{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bolinocroustibat/movies-palettes/blob/main/movies_palettes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VzeA6NjiyDe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfNR1QOyvsay"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from pathlib import Path, PosixPath\n",
        "from PIL import Image\n",
        "from skimage import feature, color\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DuMfnz7RGkXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration constants"
      ],
      "metadata": {
        "id": "Cl-babtiFgEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **`RESIZE_W` and `RESIZE_H` (Reduced Frame Size)**\n",
        "\n",
        "  This determines the resolution of each frame after downsampling. A reasonable value should:\n",
        "    - Retain enough details for meaningful clustering.\n",
        "    - Eliminate excessive computational overhead from large frame sizes.\n",
        "\n",
        "  Suggested Value: 160 x 90 (W x H)\n",
        "  - This keeps the aspect ratio of typical widescreen content (16:9).\n",
        "  -\tAt 160x90, you get 14,400 pixels per frame, which is sufficient for clustering dominant colors while ensuring faster processing.\n",
        "\n",
        "*  **`FRAME_SKIP` (Frames Skipped)**\n",
        "\n",
        "    This determines how many frames you skip between processed frames.\n",
        "  -\tMovies at 24fps typically don’t have significant color changes frame-by-frame.\n",
        "  -\tA higher skip value reduces computation but might miss some rapid scene changes.\n",
        "\n",
        "  Suggested Value: `FRAME_SKIP = 60`\n",
        " \t- This processes 1 frame per 2.5 seconds (approx.), enough to capture major color changes without excessive redundancy.\n",
        " \t-\tFor a 90-minute movie:\n",
        " \t   - Total frames at 24fps:  90 x 60 x 24 = 129,600\n",
        " \t   - With `FRAME_SKIP = 60`, you’ll process around  129,600 / 60 = 2,160  frames per movie.\n",
        "\n",
        "*  **`BATCH_SIZE` (Frames Processed at Once)**\n",
        "\n",
        "  This determines the number of frames processed in a single batch.\n",
        " \t-\tLarger batch sizes are computationally efficient as you can leverage batch processing in libraries like OpenCV.\n",
        " \t-\tHowever, too large a batch size might lead to memory limitations.\n",
        "\n",
        "  Suggested Value: `BATCH_SIZE = 20`\n",
        " \t- Processes 20 frames at a time, balancing speed and memory use.\n",
        " \t- Matches well with typical video processing setups on consumer GPUs/TPUs."
      ],
      "metadata": {
        "id": "-DlXkgiZF70G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLUSTERS_NB = 10\n",
        "FRAME_SKIP = 60  # Process one frame out of every FRAME_SKIP frames in order to speed up the process\n",
        "# Resize the image to RESIZE_W x RESIZE_H pixels in order to reduce complexity\n",
        "RESIZE_W = 160\n",
        "RESIZE_H = 90\n",
        "BATCH_SIZE = 20  # Number of frames to process in each batch"
      ],
      "metadata": {
        "id": "AcOszJudZMaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IazqExcUTobh"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxH6lzywTWCf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k88U-a-Ri2No"
      },
      "source": [
        "## Google Drive movies path and movies colors list file path\n",
        "\n",
        "> **WARNING**: Don't forget to mount Google Drive first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8lfNyToPsyz"
      },
      "outputs": [],
      "source": [
        "MOVIES_PATH = Path(\"/content/drive/MyDrive/MOVIES/\")\n",
        "FILE_PATH = Path(\"/content/drive/MyDrive/MOVIES/movies_palettes.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmLaqrEzJqBw"
      },
      "source": [
        "# Function to save movies colors list in a JSON file on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ySlHF7SKIm0"
      },
      "outputs": [],
      "source": [
        "def save_as_file(data: list, file_path: Path) -> None:\n",
        "    \"\"\"Save as JSON file on Google Drive\"\"\"\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent='\\t')\n",
        "    print(f'\"{str(file_path)}\" successfully saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP9mCshShrNN"
      },
      "source": [
        "# Build analysis file with list of dicts with movies info\n",
        "> **WARNING**: Only to be executed if the file doesn't exist yet, otherwise it will overwrite the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcQ57gEu0FDz"
      },
      "outputs": [],
      "source": [
        "# subdirs: list[PosixPath] = [p for p in MOVIES_PATH.iterdir() if p.is_dir()]\n",
        "\n",
        "# movies: list[dict] = []\n",
        "\n",
        "# for p in subdirs:\n",
        "#     # Gather some info about the movie using the folders names\n",
        "#     matches: re.Match | None = re.search(r\"(.*) \\((\\d{4})\\, (.*)\\)\", p.name)\n",
        "#     # Prepare the data structure\n",
        "#     year: str | None = None\n",
        "#     director: str | None = None\n",
        "#     file_path: str | None = None\n",
        "#     if matches:\n",
        "#         title: str = matches.group(1)\n",
        "#         year  = matches.group(2)\n",
        "#         director = matches.group(3)\n",
        "#     else:\n",
        "#       title = p.name\n",
        "\n",
        "#     # Get all video files for this movie directory\n",
        "#     file_types: tuple[str] = ('*.avi', '*.mkv', '*.mp4')\n",
        "#     files_paths: list[Path] = []\n",
        "#     for file_type in file_types:\n",
        "#         files_paths.extend(p.glob(file_type))\n",
        "\n",
        "#     # Get the unique video file path for this movie directory\n",
        "#     if len(files_paths) == 1:\n",
        "#         file_path: Path = files_paths[0]\n",
        "#         status: str = \"Movie file found\"\n",
        "#     else:\n",
        "#       if len(files_paths) == 0:\n",
        "#           status = \"No movie file found\"\n",
        "#           print(f'{status} for \"{title}\"')\n",
        "#       else:\n",
        "#           status = \"More than 1 video file found:\"\n",
        "#           for f in files_paths:\n",
        "#               status += f' \\\"{f.name}\\\"'\n",
        "#           print(status)\n",
        "\n",
        "#     movie: dict = {\n",
        "#         \"title\": title,\n",
        "#         \"status\": status,\n",
        "#         \"director\": director,\n",
        "#         \"year\": year,\n",
        "#         \"path\": str(file_path) if file_path else None,\n",
        "#         \"palettes\": [],\n",
        "#     }\n",
        "#     movies.append(movie)\n",
        "\n",
        "# # Sort alphabetically\n",
        "# movies.sort(key=lambda m: m[\"title\"])\n",
        "\n",
        "# # Save as file\n",
        "# save_as_file(data=movies, file_path=FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63VwJZRmuRz1"
      },
      "source": [
        "# Optional: analyze frames numbers and length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxDldvKPtAR5"
      },
      "outputs": [],
      "source": [
        "# for m in (pbar := tqdm(movies)):\n",
        "\n",
        "#   pbar.set_description(f'Analyzing \"{m[\"title\"]}\"')\n",
        "\n",
        "#   if m.get(\"path\") and not m.get(\"frames\"):\n",
        "#     cap = cv2.VideoCapture(str(m[\"path\"]))\n",
        "#     m[\"frames\"]: int = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "#     fps: float = cap.get(cv2.CAP_PROP_FPS)\n",
        "#     if fps:\n",
        "#       m[\"length\"]: int = int(m[\"frames\"] / fps)\n",
        "\n",
        "# save_as_file(data=movies, file_path=FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWXINNXwVrCn"
      },
      "source": [
        "# Read existing movies analysis file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGCOvIQKVqU1"
      },
      "outputs": [],
      "source": [
        "with open(FILE_PATH, \"r+\", encoding=\"utf-8\") as f:\n",
        "    movies: list[dict] = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(movies)} movies from \\\"{str(FILE_PATH)}\\\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime type"
      ],
      "metadata": {
        "id": "w_A0rEmV0wLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_runtype_type() -> str:\n",
        "  if int(os.environ.get(\"COLAB_GPU\", 0)) > 0:\n",
        "    return \"GPU\"\n",
        "  elif \"TPU_DRIVER_MODE\" in os.environ and os.environ[\"TPU_DRIVER_MODE\"] == \"tpu\":\n",
        "    return \"TPU\"\n",
        "  elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    return \"TPU\"\n",
        "  return \"unknown\""
      ],
      "metadata": {
        "id": "6UDMlB9MtGEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNY-5VRFVuWR"
      },
      "source": [
        "# Main logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tNCGJLxLN0C"
      },
      "source": [
        "This is the main logic here. It might takes a few minutes to a few hours per movie file, depending on where it's run.\n",
        "\n",
        "I suggest a GPU on Google Colab as the runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cE8M76lA8Gt"
      },
      "outputs": [],
      "source": [
        "def get_dominant_colors(\n",
        "    data: np.ndarray, clusters_nb: int\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Cluster pixels using k-means and return the dominant colors.\n",
        "\n",
        "    Parameters:\n",
        "    data (np.ndarray): A 2D array where each row is a pixel in RGB format.\n",
        "    clusters_nb (int): The number of clusters to form.\n",
        "\n",
        "    Returns:\n",
        "    tuple[np.ndarray, np.ndarray]: A tuple containing:\n",
        "        - centers (np.ndarray): The RGB values of the cluster centers.\n",
        "        - labels (np.ndarray): The label of the cluster each pixel belongs to.\n",
        "    \"\"\"\n",
        "    criteria: tuple[int, int, float] = (\n",
        "        cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,\n",
        "        10,\n",
        "        1.0,\n",
        "    )\n",
        "    flags: int = cv2.KMEANS_RANDOM_CENTERS\n",
        "    compactness: float\n",
        "    labels: np.ndarray\n",
        "    centers: np.ndarray\n",
        "    compactness, labels, centers = cv2.kmeans(\n",
        "        data.astype(np.float32), clusters_nb, None, criteria, 10, flags\n",
        "    )\n",
        "    return centers, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_salient_mask(image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates a saliency mask for the input image, highlighting the most important\n",
        "    (salient) regions based on a Spectral Residual model.\n",
        "\n",
        "    Parameters:\n",
        "    image (np.ndarray): The input color image in BGR format.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: A binary saliency map (uint8), where pixel values range from 0 to 255,\n",
        "                with high values indicating salient regions.\n",
        "    \"\"\"\n",
        "    # Convert the image to grayscale\n",
        "    gray: np.ndarray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Create a saliency detector object using the Spectral Residual model\n",
        "    saliency = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
        "\n",
        "    # Compute the saliency map\n",
        "    _, saliency_map = saliency.computeSaliency(gray)\n",
        "\n",
        "    # Return the saliency map scaled to the range [0, 255] and cast to uint8\n",
        "    return (saliency_map * 255).astype(\"uint8\")"
      ],
      "metadata": {
        "id": "IGmG2Fj8U_Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enhance_saturation(image: np.ndarray, factor: float=1.5, threshold: int = 50) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Enhance the saturation of an image and filter out low-saturation pixels.\n",
        "\n",
        "    Parameters:\n",
        "    image (np.ndarray): The input image in BGR format.\n",
        "    factor (float): The factor by which to enhance saturation.\n",
        "    threshold (int): The saturation threshold below which pixels are filtered out.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: The enhanced image.\n",
        "    \"\"\"\n",
        "    # Convert image to HSV color space\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Enhance the saturation\n",
        "    hsv[:, :, 1] = np.clip(hsv[:, :, 1] * factor, 0, 255)\n",
        "\n",
        "    # Create a mask for pixels with sufficient saturation\n",
        "    saturation_mask = hsv[:, :, 1] > threshold\n",
        "\n",
        "    # Apply the mask to filter low-saturation pixels (set to black)\n",
        "    hsv[~saturation_mask] = 0\n",
        "\n",
        "    # Convert back to BGR color space\n",
        "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)"
      ],
      "metadata": {
        "id": "IRBSkG08U0Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frame(frame: np.ndarray, clusters_nb: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    # To reduce complexity, resize the image\n",
        "    frame = cv2.resize(frame, (RESIZE_W, RESIZE_H))\n",
        "\n",
        "    # Enhance saturation\n",
        "    data = enhance_saturation(frame)\n",
        "\n",
        "    # Apply saliency detection\n",
        "    salient_mask = get_salient_mask(data)\n",
        "    salient_mask = salient_mask > 128  # Convert to binary mask\n",
        "    data = frame[salient_mask]  # Apply mask to RGB channels\n",
        "    if data.size == 0:\n",
        "        # Return empty centers and labels if no salient data exists\n",
        "        return np.empty((0, 3)), np.empty((0,))\n",
        "\n",
        "    # Convert to LAB for better perceptual clustering\n",
        "    # Generate a Numpy array of 2 dimensions, and shape of (10000, 3) (1000)\n",
        "    lab_data = cv2.cvtColor(data, cv2.COLOR_BGR2LAB).reshape(-1, 3)\n",
        "\n",
        "    # Perform k-means clustering\n",
        "    centers, labels = get_dominant_colors(lab_data, CLUSTERS_NB)\n",
        "\n",
        "    # Filter out low-luminance colors\n",
        "    luminance = centers[:, 0]\n",
        "    valid_indices = luminance > 50\n",
        "    centers = centers[valid_indices]\n",
        "    # If no valid colors remain, return empty results\n",
        "    if centers.size == 0:\n",
        "        return np.empty((0, 3)), np.empty((0,))\n",
        "\n",
        "    return centers, labels\n"
      ],
      "metadata": {
        "id": "KbifW4QnT1EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj3utgF0xivo"
      },
      "outputs": [],
      "source": [
        "def process_movie(movie: dict) -> None:\n",
        "    \"\"\"\n",
        "    Calculate the palette for a movie and save the updated movies palettes files.\n",
        "\n",
        "    Parameters:\n",
        "    movie (dict): the movie details from the movies list.\n",
        "    \"\"\"\n",
        "    start_time = time.time()  # Record the start time\n",
        "\n",
        "    cap = cv2.VideoCapture(str(m[\"path\"]))\n",
        "    frames_count: int = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    pbar.set_description(\n",
        "        f'Processing \"{m[\"title\"]}\", with {frames_count} frames.'\n",
        "    )\n",
        "\n",
        "    frame_nb = 0\n",
        "    colors: list[np.ndarray] = []\n",
        "    with tqdm(total=frames_count // FRAME_SKIP, desc=\"Frames processed\") as frame_pbar:\n",
        "        while frame_nb < frames_count:\n",
        "\n",
        "            # Extract a batch of images/frames\n",
        "            batch_frames: list[np.ndarray] = []\n",
        "            for _ in range(BATCH_SIZE):\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_nb)\n",
        "                status, frame = cap.read()\n",
        "                frame_nb += FRAME_SKIP\n",
        "                if not status:\n",
        "                  print(f\"Frame {frame_nb} could not be read. Skipping.\")\n",
        "                  break\n",
        "                batch_frames.append(frame)\n",
        "\n",
        "            if not batch_frames:\n",
        "                break\n",
        "\n",
        "            for frame in batch_frames:\n",
        "                # To reduce complexity, resize the image\n",
        "                data = cv2.resize(frame, (RESIZE_W, RESIZE_H))\n",
        "\n",
        "                # Convert the image to a list of pixels\n",
        "                # Generate a Numpy array of 2 dimensions, and shape of (10000, 3)\n",
        "                data: np.ndarray = data.reshape(-1, 3)\n",
        "\n",
        "                centers, labels = get_dominant_colors(data, CLUSTERS_NB)\n",
        "\n",
        "                # Put the CLUSTERS_NB colors in a list (will be later converted to a Numpy array)\n",
        "                # For example: [4450 2148 745 2048 609]\n",
        "                cluster_sizes: np.ndarray = np.bincount(labels.flatten())\n",
        "\n",
        "                # Sort from the largest to the smallest cluster and append to the list\n",
        "                for cluster_idx in np.argsort(-cluster_sizes):\n",
        "                    colors.append(centers[cluster_idx])\n",
        "\n",
        "            frame_pbar.update(len(batch_frames))\n",
        "\n",
        "    # Convert list of colors for this movie to a Numpy array\n",
        "    colors = np.array(colors)\n",
        "\n",
        "    if len(colors) == 0:\n",
        "      print(f\"No colors extracted for {m['title']}. Skipping.\")\n",
        "      return\n",
        "\n",
        "    # Perform K-means clustering on the colors\n",
        "    kmeans: KMeans = KMeans(n_clusters=CLUSTERS_NB, n_init=\"auto\")\n",
        "    kmeans.fit(colors)\n",
        "\n",
        "    # Get the cluster centers (representative colors)\n",
        "    cluster_centers: np.ndarray = kmeans.cluster_centers_\n",
        "\n",
        "    # Calculate the total processing duration\n",
        "    end_time: datetime = time.time()  # Record the end time\n",
        "    duration = int(end_time - start_time)  # Duration in seconds\n",
        "\n",
        "    palette: dict = {\n",
        "        # Add the parameters it used to calculate the colors\n",
        "        \"calculation_date\": datetime.now(timezone.utc).strftime(\"%Y/%m/%d_%H:%M:%S\"),\n",
        "        \"calculation_duration_seconds\": duration,\n",
        "        \"runtime\": get_runtype_type(),\n",
        "        \"clusters_nb\": CLUSTERS_NB,\n",
        "        \"frame_skip\": FRAME_SKIP,\n",
        "        \"resize\": {\"width\": RESIZE_W, \"height\": RESIZE_H},\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        # Convert the cluster centers to integers (RGB values)\n",
        "        \"colors\": cluster_centers.astype(int).tolist()\n",
        "    }\n",
        "    m[\"palettes\"].append(palette)\n",
        "\n",
        "    # Debug: display output\n",
        "    print(f'Colors for \\\"{m[\"title\"]}\\\":')\n",
        "    for color in palette[\"colors\"]:\n",
        "      img = Image.new(mode='RGB', size=(200,30), color=tuple(color))\n",
        "      display(img)\n",
        "\n",
        "    # Saving the updated list of dicts as a file\n",
        "    save_as_file(data=movies, file_path=FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final loop through movies"
      ],
      "metadata": {
        "id": "-aOcbp1Q1xQj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQmBbkMxgqT6"
      },
      "outputs": [],
      "source": [
        "RECALC_PALETTES = False # Recalculate a new palette even if it already has one\n",
        "\n",
        "for m in (pbar := tqdm(movies)):\n",
        "    if len(m[\"palettes\"]) > 0 and not RECALC_PALETTES:\n",
        "        print(f'\"{m[\"title\"]}\" already has at least one color palette calculated, skipping...')\n",
        "        continue\n",
        "\n",
        "    if not m.get(\"path\"):\n",
        "        print(f'\"{m[\"title\"]}\" has no filepath, skipping...')\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "      process_movie(movie=m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cim2Z_WAW887"
      },
      "source": [
        "# Test: display colors palettes for each movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7evqEOXX9LQ"
      },
      "outputs": [],
      "source": [
        "for m in (pbar := tqdm(movies)):\n",
        "    if len(m.get(\"palettes\")) > 0:\n",
        "        print(m[\"title\"])\n",
        "        for p in m[\"palettes\"]:\n",
        "          print(f'\\nPalette calculated on {p.get(\"calculation_date\", \"unknown\")}:')\n",
        "          for color in p[\"colors\"]:\n",
        "            img = Image.new(mode='RGB', size=(200,30), color=tuple(color))\n",
        "            display(img)\n",
        "        print(\"\\n-------------------------------------\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GP9mCshShrNN",
        "63VwJZRmuRz1",
        "w_A0rEmV0wLF"
      ],
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}