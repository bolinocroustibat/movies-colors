{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bolinocroustibat/movies-palettes/blob/main/movies_palettes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VzeA6NjiyDe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfNR1QOyvsay"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import hashlib\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import sqlite3\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from dateutil import parser\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from pathlib import Path, PosixPath\n",
        "from PIL import Image\n",
        "from skimage import feature, color\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl-babtiFgEL"
      },
      "source": [
        "# Configuration constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DlXkgiZF70G"
      },
      "source": [
        "*   **`RESIZE_W` and `RESIZE_H` (Reduced Frame Size)**\n",
        "\n",
        "  This determines the resolution of each frame after downsampling. A reasonable value should:\n",
        "    - Retain enough details for meaningful clustering.\n",
        "    - Eliminate excessive computational overhead from large frame sizes.\n",
        "\n",
        "  Suggested Value: 160 x 90 (W x H)\n",
        "  - This keeps the aspect ratio of typical widescreen content (16:9).\n",
        "  -\tAt 160x90, you get 14,400 pixels per frame, which is sufficient for clustering dominant colors while ensuring faster processing.\n",
        "\n",
        "*  **`FRAME_SKIP` (Frames Skipped)**\n",
        "\n",
        "    This determines how many frames you skip between processed frames.\n",
        "  -\tMovies at 24fps typically don’t have significant color changes frame-by-frame.\n",
        "  -\tA higher skip value reduces computation but might miss some rapid scene changes.\n",
        "\n",
        "  Suggested Value: `FRAME_SKIP = 60`\n",
        " \t- This processes 1 frame per 2.5 seconds (approx.), enough to capture major color changes without excessive redundancy.\n",
        " \t-\tFor a 90-minute movie:\n",
        " \t   - Total frames at 24fps:  90 x 60 x 24 = 129,600\n",
        " \t   - With `FRAME_SKIP = 60`, you’ll process around  129,600 / 60 = 2,160  frames per movie.\n",
        "\n",
        "*  **`BATCH_SIZE` (Frames Processed at Once)**\n",
        "\n",
        "  This determines the number of frames processed in a single batch.\n",
        " \t-\tLarger batch sizes are computationally efficient as you can leverage batch processing in libraries like OpenCV.\n",
        " \t-\tHowever, too large a batch size might lead to memory limitations.\n",
        "\n",
        "  Suggested Values:\n",
        "  - For CPU processing, you can try `BATCH_SIZE = 20`.\n",
        "  - For an NVIDIA A100 GPU, which has 40/80GB of VRAM depending on the variant, you can use a significantly larger batch, like `BATCH_SIZE = 32`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcOszJudZMaZ"
      },
      "outputs": [],
      "source": [
        "CLUSTERS_NB = 10\n",
        "CLUSTERS_NB_BW = 4\n",
        "\n",
        "FRAME_SKIP = 60  # Process one frame out of every FRAME_SKIP frames in order to speed up the process\n",
        "# Resize the image to RESIZE_W x RESIZE_H pixels in order to reduce complexity\n",
        "RESIZE_W = 160\n",
        "RESIZE_H = 90\n",
        "BATCH_SIZE = 32  # Number of frames to process in each batch\n",
        "\n",
        "METHOD = \"cv2\" # \"cv2\" or \"sklearn\". sklearn is supposedly more precise, but cv2 seems to give better results\n",
        "COLOR_SPACE = \"RGB\"  # \"RGB\" or \"LAB\"\n",
        "SATURATION_FACTOR = 1.0  # Default value of 1.0 means no saturation adjustment\n",
        "\n",
        "# Recalculate a new palette even if it already has one if the palette is older than this.\n",
        "# Put None if you want to recalculate no matter what\n",
        "RECALC_IF_BEFORE = None\n",
        "# Calculate only a specific movie\n",
        "MOVIE_TO_PROCESS = \"Dick Tracy\"  # Set to None to process all movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IazqExcUTobh"
      },
      "source": [
        "# Mount database from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxH6lzywTWCf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "SQLITE_FILE_PATH = Path(\"/content/drive/MyDrive/MOVIES/movies.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_A0rEmV0wLF"
      },
      "source": [
        "# Function to get runtime type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UDMlB9MtGEY"
      },
      "outputs": [],
      "source": [
        "def get_runtype_type() -> str:\n",
        "  if int(os.environ.get(\"COLAB_GPU\", 0)) > 0:\n",
        "    return \"GPU\"\n",
        "  elif \"TPU_DRIVER_MODE\" in os.environ and os.environ[\"TPU_DRIVER_MODE\"] == \"tpu\":\n",
        "    return \"TPU\"\n",
        "  elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    return \"TPU\"\n",
        "  return \"unknown\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkbxsdLa9mlc"
      },
      "source": [
        "# Function to build a UUID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FdmQHY49q9M"
      },
      "outputs": [],
      "source": [
        "def generate_palette_id(title: str, calculation_date: str) -> str:\n",
        "    \"\"\"Generate a short unique identifier for a palette\"\"\"\n",
        "    unique_string = f\"{title}_{calculation_date}\"\n",
        "    hash_object = hashlib.md5(unique_string.encode())\n",
        "    return hash_object.hexdigest()[:6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNY-5VRFVuWR"
      },
      "source": [
        "# Main logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tNCGJLxLN0C"
      },
      "source": [
        "This is the main logic here. It might takes a few minutes to a few hours per movie file, depending on where it's run.\n",
        "\n",
        "I suggest a GPU on Google Colab as the runtime.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "815Jnr0twumo"
      },
      "outputs": [],
      "source": [
        "def is_black_and_white(cap: cv2.VideoCapture, threshold: float = 0.1) -> bool:\n",
        "    \"\"\"\n",
        "    Determine if a movie is black and white based on the average saturation.\n",
        "\n",
        "    Parameters:\n",
        "    cap (cv2.VideoCapture): The video capture object.\n",
        "    threshold (float): The saturation threshold below which the movie is considered B&W.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the movie is black and white, False otherwise.\n",
        "    \"\"\"\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    sample_frames = min(total_frames, 100)  # Sample up to 100 frames\n",
        "\n",
        "    saturation_values = []\n",
        "    for i in range(0, total_frames, max(1, total_frames // sample_frames)):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "        saturation = hsv[:, :, 1].mean() / 255.0  # Normalize to [0,1]\n",
        "        saturation_values.append(saturation)\n",
        "\n",
        "    average_saturation = np.mean(saturation_values)\n",
        "    return average_saturation < threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cE8M76lA8Gt"
      },
      "outputs": [],
      "source": [
        "def get_dominant_colors_cv2(\n",
        "    data: np.ndarray, clusters_nb: int, color_space: str = \"RGB\"\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"OpenCV version with optional LAB color space, returns float64 centers\"\"\"\n",
        "    if color_space == \"LAB\":\n",
        "        lab_data = cv2.cvtColor(data.reshape(-1, 1, 3), cv2.COLOR_RGB2LAB).reshape(\n",
        "            -1, 3\n",
        "        )\n",
        "        clustering_data = lab_data.astype(np.float32)\n",
        "    else:\n",
        "        clustering_data = data.astype(np.float32)\n",
        "\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
        "\n",
        "    _, labels, centers = cv2.kmeans(\n",
        "        clustering_data, clusters_nb, None, criteria, 10, flags\n",
        "    )\n",
        "\n",
        "    if color_space == \"LAB\":\n",
        "        # Convert to uint8 for cv2.cvtColor, then back to float64\n",
        "        centers_rgb = cv2.cvtColor(\n",
        "            centers.reshape(-1, 1, 3).astype(np.uint8), cv2.COLOR_LAB2RGB\n",
        "        )\n",
        "        centers = centers_rgb.reshape(-1, 3).astype(np.float64)\n",
        "    else:\n",
        "        # Just convert to float64\n",
        "        centers = centers.astype(np.float64)\n",
        "\n",
        "    return centers, labels.ravel()\n",
        "\n",
        "def get_dominant_colors_sklearn(\n",
        "    data: np.ndarray, clusters_nb: int, color_space: str = \"RGB\"\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Cluster pixels using k-means and return the dominant colors.\n",
        "    Will automatically reduce number of clusters if not enough distinct clusters are found.\n",
        "\n",
        "    Parameters:\n",
        "    data (np.ndarray): A 2D array where each row is a pixel in RGB format.\n",
        "    clusters_nb (int): The maximum number of clusters to form.\n",
        "\n",
        "    Returns:\n",
        "    tuple[np.ndarray, np.ndarray]: A tuple containing:\n",
        "        - centers (np.ndarray): The RGB values of the cluster centers.\n",
        "        - labels (np.ndarray): The label of the cluster each pixel belongs to.\n",
        "    \"\"\"\n",
        "    if color_space == \"LAB\":\n",
        "        # Convert to LAB color space\n",
        "        lab_data = cv2.cvtColor(data.reshape(-1, 1, 3), cv2.COLOR_RGB2LAB).reshape(\n",
        "            -1, 3\n",
        "        )\n",
        "        clustering_data = lab_data\n",
        "    else:\n",
        "        clustering_data = data\n",
        "\n",
        "    kmeans = KMeans(n_clusters=clusters_nb, n_init=\"auto\")\n",
        "    labels = kmeans.fit_predict(clustering_data)\n",
        "    centers = kmeans.cluster_centers_\n",
        "\n",
        "    if color_space == \"LAB\":\n",
        "        # Convert centers back to RGB\n",
        "        centers_rgb = cv2.cvtColor(\n",
        "            centers.reshape(-1, 1, 3).astype(np.uint8), cv2.COLOR_LAB2RGB\n",
        "        )\n",
        "        centers = centers_rgb.reshape(-1, 3)\n",
        "\n",
        "    return centers, labels\n",
        "\n",
        "\n",
        "def get_dominant_luminances(\n",
        "    data: np.ndarray, clusters_nb: int\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Find the dominant luminance values in an image using k-means clustering.\n",
        "    Returns these luminance values as grayscale RGB values (where R=G=B).\n",
        "\n",
        "    Parameters:\n",
        "    data (np.ndarray): Input image (can be color or grayscale)\n",
        "    clusters_nb (int): Number of luminance levels to identify\n",
        "\n",
        "    Returns:\n",
        "    tuple[np.ndarray, np.ndarray]: A tuple containing:\n",
        "        - centers (np.ndarray): Array of shape (clusters_nb, 3) where each row is\n",
        "          a grayscale RGB value [v,v,v] representing a dominant luminance level\n",
        "        - labels (np.ndarray): Array indicating which cluster each pixel belongs to\n",
        "    \"\"\"\n",
        "    # Convert to grayscale if input is color image\n",
        "    if len(data.shape) > 2:\n",
        "        data = cv2.cvtColor(data, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Reshape to 1D array for clustering\n",
        "    data_reshaped = data.reshape(-1, 1).astype(np.float32)\n",
        "\n",
        "    # Define k-means parameters\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
        "\n",
        "    # Perform k-means clustering on luminance values\n",
        "    _, labels, centers = cv2.kmeans(\n",
        "        data_reshaped, clusters_nb, None, criteria, 10, flags\n",
        "    )\n",
        "\n",
        "    # Sort centers from darkest to lightest\n",
        "    sort_idx = np.argsort(centers.flatten())\n",
        "    centers = centers[sort_idx]\n",
        "\n",
        "    # Convert luminance values to RGB format (all channels equal)\n",
        "    # e.g., luminance value 127 becomes [127, 127, 127]\n",
        "    centers_rgb = np.column_stack([centers.flatten()] * 3)\n",
        "\n",
        "    return centers_rgb, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ermSYWaBUboG"
      },
      "outputs": [],
      "source": [
        "def process_batch(frames: list[np.ndarray], is_bw: bool) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Process a batch of frames at once to extract dominant colors.\n",
        "\n",
        "    Parameters:\n",
        "    frames (list): List of frames to process\n",
        "    is_bw (bool): Whether to process as black and white\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: Array of dominant colors found across all frames\n",
        "    \"\"\"\n",
        "    # Stack all frames into a single array\n",
        "    batch = np.stack(frames)\n",
        "\n",
        "    # Resize all frames at once\n",
        "    resized = np.array([cv2.resize(f, (RESIZE_W, RESIZE_H)) for f in batch])\n",
        "\n",
        "    if is_bw:\n",
        "        # Convert to grayscale and process all frames\n",
        "        gray_batch = np.array([cv2.cvtColor(f, cv2.COLOR_BGR2GRAY) for f in resized])\n",
        "        all_pixels = gray_batch.reshape(-1, 1)\n",
        "        centers, labels = get_dominant_luminances(all_pixels, CLUSTERS_NB_BW)\n",
        "    else:\n",
        "        # Convert to RGB and process all frames\n",
        "        rgb_batch = np.array([cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in resized])\n",
        "\n",
        "        # Apply saturation adjustment only if factor is not 1.0\n",
        "        if SATURATION_FACTOR != 1.0:\n",
        "            hsv_batch = np.array([cv2.cvtColor(f, cv2.COLOR_RGB2HSV) for f in rgb_batch])\n",
        "            hsv_batch[:, :, :, 1] = hsv_batch[:, :, :, 1] * SATURATION_FACTOR\n",
        "            hsv_batch[:, :, :, 1] = np.clip(hsv_batch[:, :, :, 1], 0, 255)\n",
        "            rgb_batch = np.array([cv2.cvtColor(f, cv2.COLOR_HSV2RGB) for f in hsv_batch])\n",
        "\n",
        "        # Reshape to process all pixels from all frames\n",
        "        all_pixels = rgb_batch.reshape(-1, 3)\n",
        "\n",
        "        if METHOD == \"cv2\":\n",
        "            centers, labels = get_dominant_colors_cv2(all_pixels, CLUSTERS_NB, COLOR_SPACE)\n",
        "        elif METHOD == \"sklearn\":\n",
        "            centers, labels = get_dominant_colors_sklearn(all_pixels, CLUSTERS_NB, COLOR_SPACE)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {METHOD}\")\n",
        "\n",
        "    # Sort clusters by frequency\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    sorted_idx = np.argsort(-counts)  # Sort in descending order\n",
        "    centers = centers[sorted_idx]\n",
        "\n",
        "    return centers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj3utgF0xivo"
      },
      "outputs": [],
      "source": [
        "def process_movie(movie_title: str, file_path: Path) -> dict | None:\n",
        "    \"\"\"\n",
        "    Calculate the palette for a movie.\n",
        "\n",
        "    This function processes a movie file to extract its dominant color palette.\n",
        "    For black and white movies, it extracts dominant luminance values instead.\n",
        "    The processing is done in batches to improve performance.\n",
        "\n",
        "    Parameters:\n",
        "    - path: Path to the movie file\n",
        "    - title: Movie title\n",
        "    Returns:\n",
        "    dict: The new palette entry\n",
        "    \"\"\"\n",
        "    start_time: float = time.time()\n",
        "\n",
        "    pbar.set_description(f'Processing movie \"{movie_title}\"...')\n",
        "\n",
        "    cap = cv2.VideoCapture(str(file_path))\n",
        "    frames_count: int = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frames_to_process: int = frames_count // FRAME_SKIP\n",
        "    if frames_to_process < 1:\n",
        "        print(f'Movie \"{movie_title}\" has less than {FRAME_SKIP} frames, skipping...')\n",
        "        return None\n",
        "\n",
        "    # Detect if movie is black and white\n",
        "    is_bw: bool = is_black_and_white(cap)\n",
        "    if is_bw:\n",
        "        print(f'Movie \"{movie_title}\" is black and white, with {frames_count} frames.')\n",
        "    else:\n",
        "        print(f'Movie \"{movie_title}\" is color, with {frames_count} frames.')\n",
        "\n",
        "    frame_nb = 0\n",
        "    all_centers = []\n",
        "    frames_processed = 0\n",
        "\n",
        "    with tqdm(total=frames_to_process, desc=\"Frames processed\") as frame_pbar:\n",
        "        while frame_nb < frames_count:\n",
        "            batch_frames = []\n",
        "            frames_read = 0\n",
        "\n",
        "            # Calculate how many frames we still need to process\n",
        "            remaining_frames = frames_to_process - frames_processed\n",
        "            current_batch_size = min(BATCH_SIZE, remaining_frames)\n",
        "\n",
        "            # Read up to current_batch_size frames\n",
        "            for _ in range(current_batch_size):\n",
        "                if frame_nb >= frames_count:\n",
        "                    break\n",
        "\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_nb)\n",
        "                status, frame = cap.read()\n",
        "\n",
        "                if not status:\n",
        "                    frame_nb += FRAME_SKIP\n",
        "                    continue\n",
        "\n",
        "                batch_frames.append(frame)\n",
        "                frames_read += 1\n",
        "                frame_nb += FRAME_SKIP\n",
        "\n",
        "            if not batch_frames:\n",
        "                break\n",
        "\n",
        "            # Process batch and extend results\n",
        "            batch_centers = process_batch(batch_frames, is_bw)\n",
        "            all_centers.extend(batch_centers)\n",
        "            frame_pbar.update(frames_read)\n",
        "            frames_processed += frames_read\n",
        "\n",
        "    if len(all_centers) == 0:\n",
        "        print(f\"No colors extracted for {movie_title}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # Combine all centers and do final clustering\n",
        "    all_centers = np.vstack(all_centers)\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=CLUSTERS_NB if not is_bw else CLUSTERS_NB_BW, n_init=\"auto\"\n",
        "    )\n",
        "    kmeans.fit(all_centers)\n",
        "    final_colors: np.ndarray = kmeans.cluster_centers_\n",
        "    final_palette = [tuple(map(int, c)) for c in final_colors]\n",
        "\n",
        "    # Calculate the total processing duration\n",
        "    end_time: float = time.time()\n",
        "    duration: int = int(end_time - start_time)\n",
        "    # Calculation date\n",
        "    now: str = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "    return {\n",
        "        \"id\": generate_palette_id(movie_title, now),\n",
        "        \"calculation_date\": now,\n",
        "        \"calculation_duration_seconds\": duration,\n",
        "        \"is_black_and_white\": is_bw,\n",
        "        \"colors\": final_palette,\n",
        "        \"clusters_nb\": CLUSTERS_NB_BW if is_bw else CLUSTERS_NB,\n",
        "        \"frame_skip\": FRAME_SKIP,\n",
        "        \"resize_width\": RESIZE_W,\n",
        "        \"resize_height\": RESIZE_H,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"clustering_method\": METHOD,\n",
        "        \"color_space\": COLOR_SPACE,\n",
        "        \"saturation_factor\": SATURATION_FACTOR,\n",
        "        \"runtime\": get_runtype_type()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbb3DT51S_1E"
      },
      "source": [
        "# Display palette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtqqCPASTCa9"
      },
      "outputs": [],
      "source": [
        "def display_palette(palette: dict) -> None:\n",
        "    # Display metadata\n",
        "    for k, v in palette.items():\n",
        "        if k != \"colors\":  # Skip colors as we'll display them separately\n",
        "            print(f'{k}: {v}')\n",
        "\n",
        "    # Display color swatches\n",
        "    for c in palette[\"colors\"]:\n",
        "        img = Image.new(mode=\"RGB\", size=(200, 30), color=tuple(c))\n",
        "        display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aOcbp1Q1xQj"
      },
      "source": [
        "# Final loop through movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQmBbkMxgqT6"
      },
      "outputs": [],
      "source": [
        "with sqlite3.connect(SQLITE_FILE_PATH) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT\n",
        "            m.id,\n",
        "            m.title,\n",
        "            m.path,\n",
        "            JSON_GROUP_ARRAY(p.calculation_date) as palettes\n",
        "        FROM movies m\n",
        "        LEFT JOIN palettes p ON m.id = p.movie_id\n",
        "        WHERE m.type = 'movie'\n",
        "        GROUP BY m.id, m.title;\n",
        "    \"\"\")\n",
        "    movies = cursor.fetchall()\n",
        "    print(f'Found {len(movies)} movies to process.')\n",
        "\n",
        "    for m in (pbar := tqdm(movies)):\n",
        "        # Skip movies not matching MOVIE_TO_PROCESS, if specified\n",
        "        if MOVIE_TO_PROCESS and m[1] != MOVIE_TO_PROCESS:\n",
        "            continue\n",
        "\n",
        "        # Check if movie already has palettes and if any are newer than RECALC_IF_BEFORE\n",
        "        if RECALC_IF_BEFORE is not None and m[3] is not None and m[3] != '[null]':\n",
        "            calculation_dates: list[str] = json.loads(m[3])\n",
        "            skip_movie: bool = False\n",
        "            for date_str in calculation_dates:\n",
        "                if date_str is not None and date_str != \"\":\n",
        "                    calculation_date = parser.parse(date_str)\n",
        "                    if calculation_date > RECALC_IF_BEFORE:\n",
        "                        skip_movie = True\n",
        "                        break  # No need to check further dates\n",
        "\n",
        "            if skip_movie:\n",
        "                # tqdm.write(f'\"{m[1]}\" already has a recent color palette, skipping...')\n",
        "                continue\n",
        "\n",
        "        if not m[2]:\n",
        "            # tqdm.write(f'\"{m[1]}\" has no filepath, skipping...')\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            palette: dict = process_movie(movie_title=m[1], file_path=Path(m[2]))\n",
        "            if palette is None:\n",
        "                continue\n",
        "\n",
        "            # Debug: display new palette\n",
        "            tqdm.write(f'Calculated palette for \"{m[1]}\"')\n",
        "            display_palette(palette)\n",
        "\n",
        "            # Insert the new palette into the DB\n",
        "            sql_query = \"\"\"\n",
        "            INSERT OR REPLACE INTO palettes (\n",
        "                id,\n",
        "                movie_id,\n",
        "                calculation_date,\n",
        "                calculation_duration_seconds,\n",
        "                is_black_and_white,\n",
        "                colors,\n",
        "                clusters_nb,\n",
        "                frame_skip,\n",
        "                resize_width,\n",
        "                resize_height,\n",
        "                batch_size,\n",
        "                clustering_method,\n",
        "                color_space,\n",
        "                saturation_factor,\n",
        "                runtime\n",
        "            )\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\"\n",
        "            data_to_insert = (\n",
        "                palette[\"id\"],\n",
        "                m[0],\n",
        "                palette[\"calculation_date\"],\n",
        "                palette[\"calculation_duration_seconds\"],\n",
        "                palette[\"is_black_and_white\"],\n",
        "                json.dumps(palette.get(\"colors\", [])),\n",
        "                palette[\"clusters_nb\"],\n",
        "                palette[\"frame_skip\"],\n",
        "                palette[\"resize_width\"],\n",
        "                palette[\"resize_height\"],\n",
        "                palette[\"batch_size\"],\n",
        "                palette[\"clustering_method\"],\n",
        "                palette[\"color_space\"],\n",
        "                palette[\"saturation_factor\"],\n",
        "                palette[\"runtime\"],\n",
        "            )\n",
        "            cursor.execute(sql_query, data_to_insert)\n",
        "            conn.commit()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}